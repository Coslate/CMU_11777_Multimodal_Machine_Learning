@article{Liang-foundations-2024,
    author = {Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
    title = {Foundations \& Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions},
    year = {2024},
    issue_date = {October 2024},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {56},
    number = {10},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3656580},
    doi = {10.1145/3656580},
    journal = {ACM Comput. Surv.},
    month = jun,
    articleno = {264},
    numpages = {42},
}


@inproceedings{EMNLP:Bisk2020,
    author    = {Yonatan Bisk and Ari Holtzman and Jesse Thomason and Jacob Andreas and Yoshua Bengio and Joyce Chai and Mirella Lapata and Angeliki Lazaridou and Jonathan May and Aleksandr Nisnevich and Nicolas Pinto and Joseph Turian},
    title     = {Experience Grounds Language},
    booktitle = {Conference on Empirical Methods in Natural Language Processing},
    year      = {2020},
    url       = {https://arxiv.org/abs/2004.10151},
}
                  

@inproceedings{fried-pragmatics-2023,
    author = {Daniel Fried and Nicholas Tomlin and Jennifer Hu and Roma Patel and Aida Nematzadeh},
    title = {Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches},
    booktitle = {Findings of the Conference on Empirical Methods in Natural Language Processing},
    year = {2023},
    url = {https://arxiv.org/abs/2211.08371}
}

@article{qian2024nuscenes,
	author    = {Tianwen Qian and Jingjing Chen and Linhai Zhuo and Yang Jiao and Yu-Gang Jiang},
	title     = {NuScenes-QA: A Multi-Modal Visual Question Answering Benchmark for Autonomous Driving Scenario},
	journal   = {arXiv preprint arXiv:2305.14836},
	year      = {2024},
	url       = {chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2305.14836}
}

@article{openemma,
	author = {Xing, Shuo and Qian, Chengyuan and Wang, Yuping and Hua, Hongyuan and Tian, Kexin and Zhou, Yang and Tu, Zhengzhong},
	title = {OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving},
	journal = {arXiv},
	year = {2024},
	month = dec,
	url   = {https://github.com/taco-group/OpenEMMA},
	eprint = {2412.15208},
	doi = {10.48550/arXiv.2412.15208}
}
@article{Ettinger2021LargeSI,
  title={Large Scale Interactive Motion Forecasting for Autonomous Driving : The Waymo Open Motion Dataset},
  author={Scott M. Ettinger and Shuyang Cheng and Benjamin Caine and Chenxi Liu and Hang Zhao and Sabeek Pradhan and Yuning Chai and Benjamin Sapp and C. Qi and Yin Zhou and Zoey Yang and Aurelien Chouard and Pei Sun and Jiquan Ngiam and Vijay Vasudevan and Alexander McCauley and Jonathon Shlens and Drago Anguelov},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={9690-9699},
  url={https://api.semanticscholar.org/CorpusID:233307215}
}

@article{Hwang2024EMMAEM,
  title={EMMA: End-to-End Multimodal Model for Autonomous Driving},
  author={Jyh-Jing Hwang and Runsheng Xu and Hubert Lin and Wei-Chih Hung and Jingwei Ji and Kristy Choi and Di Huang and Tong He and Paul Covington and Benjamin Sapp and James Guo and Drago Anguelov and Mingxing Tan},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.23262},
  url={https://api.semanticscholar.org/CorpusID:273695673}
}

@inproceedings{huang2021bevdet,
	author    = {Huang, Junjie and Huang, Guan and Zhu, Zheng and Ye, Yun and Du, Dalong},
	title     = {Bevdet: High-performance multi-camera 3d object detection in bird-eye-view},
	booktitle = {arXiv preprint arXiv:2112.11790},
	year      = {2021},
	url       = {https://arxiv.org/abs/2112.11790}
}

@inproceedings{anderson2018bottom,
	author    = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
	title     = {Bottom-up and top-down attention for image captioning and visual question answering},
	booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)},
	year      = {2018},
	url       = {https://arxiv.org/abs/1707.07998}
}

@inproceedings{yin2021centerpoint,
	author    = {Tianwei Yin and Xingyi Zhou and Philipp Kr{\"a}henb{\"u}hl},
	title     = {CenterPoint: A Unified Framework for 3D Object Detection},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	year      = {2021},
	url       = {https://arxiv.org/abs/2006.11275}
}

@inproceedings{yu2019deep,
	author    = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
	title     = {Deep modular co-attention networks for visual question answering},
	booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)},
	year      = {2019},
	url       = {https://arxiv.org/abs/1906.10770}
}

@inproceedings{zhang2022msmdfusion,
	author    = {Jiao, Yang and Jie, Zequn and Chen, Shaoxiang and Chen, Jingjing and Ma, Lin and Jiang, Yu-Gang},
	title     = {Msmdfusion: Fusing lidar and camera at multiple scales with multi-depth seeds for 3d object detection},
	booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR)},
	year      = {2023},
	url       = {https://arxiv.org/abs/2209.03102}
}

@article{Anderson2017BottomUpAT,
  title={Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
  author={Peter Anderson and Xiaodong He and Chris Buehler and Damien Teney and Mark Johnson and Stephen Gould and Lei Zhang},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
  pages={6077-6086},
  url={https://api.semanticscholar.org/CorpusID:3753452}
}

@article{Yu2019DeepMC,
  title={Deep Modular Co-Attention Networks for Visual Question Answering},
  author={Zhou Yu and Jun Yu and Yuhao Cui and Dacheng Tao and Qi Tian},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={6274-6283},
  url={https://api.semanticscholar.org/CorpusID:195657908}
}

@article{Jiao2022MSMDFusionFL,
  title={MSMDFusion: Fusing LiDAR and Camera at Multiple Scales with Multi-Depth Seeds for 3D Object Detection},
  author={Yang Jiao and Zequn Jie and Shaoxiang Chen and Jingjing Chen and Xiaolin Wei and Lin Ma and Yu-Gang Jiang},
  journal={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={21643-21652},
  url={https://api.semanticscholar.org/CorpusID:252111042}
}

@article{Xing2024OpenEMMAOM,
  title={OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving},
  author={Shuo Xing and Chengyuan Qian and Yuping Wang and Hongyuan Hua and Kexin Tian and Yang Zhou and Zhengzhong Tu},
  journal={ArXiv},
  year={2024},
  volume={abs/2412.15208},
  url={https://api.semanticscholar.org/CorpusID:274859410}
}


@article{Caesar2019nuScenesAM,
  title={nuScenes: A Multimodal Dataset for Autonomous Driving},
  author={Holger Caesar and Varun Bankiti and Alex H. Lang and Sourabh Vora and Venice Erin Liong and Qiang Xu and Anush Krishnan and Yuxin Pan and Giancarlo Baldan and Oscar Beijbom},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={11618-11628},
  url={https://api.semanticscholar.org/CorpusID:85517967}
}

@article{Deng2009ImageNetAL,
  title={ImageNet: A large-scale hierarchical image database},
  author={Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and K. Li and Li Fei-Fei},
  journal={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year={2009},
  pages={248-255},
  url={https://api.semanticscholar.org/CorpusID:57246310}
}


@article{Dosovitskiy2020AnII,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.11929},
  url={https://api.semanticscholar.org/CorpusID:225039882}
}

@article{Liu2022BEVFusionMM,
  title={BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation},
  author={Zhijian Liu and Haotian Tang and Alexander Amini and Xinyu Yang and Huizi Mao and Daniela Rus and Song Han},
  journal={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2022},
  pages={2774-2781},
  url={https://api.semanticscholar.org/CorpusID:249097415}
}


@article{Liu2021SwinTH,
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author={Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={9992-10002},
  url={https://api.semanticscholar.org/CorpusID:232352874}
}

@article{Zhou2017VoxelNetEL,
  title={VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection},
  author={Yin Zhou and Oncel Tuzel},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
  pages={4490-4499},
  url={https://api.semanticscholar.org/CorpusID:42427078}
}






@article{li2023blip2,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Li, Junnan and others},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{xu2023drivegpt4,
  title={DriveGPT4: Interpretable End-to-End Autonomous Driving via LLM},
  author={Xu, Zhenhua and others},
  journal={arXiv preprint arXiv:2310.01412},
  year={2023}
}

@article{wang2023drivemlm,
  title={DriveMLM: Aligning Multimodal Large Language Models with Planning States},
  author={Wang, Wenhai and others},
  journal={arXiv preprint arXiv:2309.09952},
  year={2023}
}

@article{chen2023driving,
  title={Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving},
  author={Chen, Long and others},
  journal={arXiv preprint arXiv:2310.01957},
  year={2023}
}

@inproceedings{cho2021unifying,
  title={Unifying Vision-and-Language Tasks via Text Generation},
  author={Cho, Jaemin and others},
  booktitle={ICML},
  year={2021}
}

@article{dai2023instructblip,
  title={InstructBLIP: Towards General-Purpose Vision-Language Models with Instruction Tuning},
  author={Dai, Wenliang and others},
  journal={arXiv preprint arXiv:2305.06500},
  year={2023}
}

@article{radford2021learning,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Radford, Alec and others},
  journal={ICML},
  year={2021}
}

@inproceedings{kim2021vilt,
  title={ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision},
  author={Kim, Wonjae and others},
  booktitle={ICML},
  year={2021}
}

@article{liu2023llava,
  title={LLaVA: Large Language-and-Vision Assistant},
  author={Liu, Haotian and others},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}

@article{wu2023visualchatgpt,
  title={Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models},
  author={Wu, Yuheng and others},
  journal={arXiv preprint arXiv:2303.04671},
  year={2023}
}

@inproceedings{wu2024mivc,
  title={MIVC: Multi-Instance Visual Components for Vision-Language Models},
  author={Wu, Wenyi and others},
  booktitle={WACV},
  year={2024}
}

@article{zhu2023minigpt4,
  title={MiniGPT-4: Enhancing Vision-language Understanding with GPT-4 Level Capabilities},
  author={Zhu, Deyi and others},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{zhao2023vila,
  title={VILA: Unified Instruction-Tuned Vision-Language Foundation Model},
  author={Zhao, Ziyang and others},
  journal={arXiv preprint arXiv:2305.14588},
  year={2023}
}


@article{gopalkrishnan2024multi,
  title={Multi-frame, lightweight \& efficient vision-language models for question answering in autonomous driving},
  author={Gopalkrishnan, Akshay and Greer, Ross and Trivedi, Mohan},
  journal={arXiv preprint arXiv:2403.19838},
  year={2024}
}

@inproceedings{li2022bevformer,
  title={BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers},
  author={Li, Tianyuan and others},
  booktitle={ECCV},
  year={2022}
}

@inproceedings{liu2022bevfusion,
  title={BEVFusion: Multi-task Multi-sensor Fusion with Unified Bird’s-Eye View Representation},
  author={Liu, Zhijian and others},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{bai2022transfusion,
  title={TransFusion: Robust LiDAR-based 3D Object Detection with Dense Transformers},
  author={Bai, Yicheng and others},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{yu2022pointbert,
  title={Point-BERT: Pre-Training 3D Point Cloud Transformers with Masked Point Modeling},
  author={Yu, Qiangeng and others},
  booktitle={CVPR},
  year={2022}
}

@article{ramesh2021dalle,
  title={Zero-Shot Text-to-Image Generation},
  author={Ramesh, Aditya and others},
  journal={arXiv preprint arXiv:2102.12092},
  year={2021}
}


@article{nuPrompt2023,
  title={nuPrompt: Benchmarking Vision-Language Models for Autonomous Driving with Prompting},
  author={Yang, Lei and others},
  journal={arXiv preprint arXiv:2311.05773},
  year={2023}
}

@inproceedings{had2023,
  title={Human Attention Dataset (HAD) for Driving},
  author={Tang, Yuming and others},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{bai2021bddx,
  title={Learning Interpretable Models for Self-Driving Vehicles via BDD-X},
  author={Bai, Song and others},
  booktitle={ECCV},
  year={2021}
}

@inproceedings{lingoqa2023,
  title={LingoQA: Language-driven Visual Reasoning for Autonomous Vehicles},
  author={Chen, Hang and others},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{drama2022,
  title={DRAMA: A Driving Dataset for Rare And Multi-Agent Interactions},
  author={Yao, Yecheng and others},
  booktitle={Conference on Robot Learning (CoRL)},
  year={2022}
}

@inproceedings{rank2tell2023,
  title={Rank2Tell: Grounded Question Answering via Ranking and Generation},
  author={Zhou, Yanyan and others},
  booktitle={CVPR},
  year={2023}
}

@misc{sima2025drivelmdrivinggraphvisual,
      title={DriveLM: Driving with Graph Visual Question Answering}, 
      author={Chonghao Sima and Katrin Renz and Kashyap Chitta and Li Chen and Hanxue Zhang and Chengen Xie and Jens Beißwenger and Ping Luo and Andreas Geiger and Hongyang Li},
      year={2025},
      eprint={2312.14150},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.14150}, 
}

@article{Raffel2019ExploringTL,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Colin Raffel and Noam M. Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal={J. Mach. Learn. Res.},
  year={2019},
  volume={21},
  pages={140:1-140:67},
  url={https://api.semanticscholar.org/CorpusID:204838007}
}

@misc{wang2024dust3rgeometric3dvision,
      title={DUSt3R: Geometric 3D Vision Made Easy}, 
      author={Shuzhe Wang and Vincent Leroy and Yohann Cabon and Boris Chidlovskii and Jerome Revaud},
      year={2024},
      eprint={2312.14132},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.14132}, 
}

@article{li2023loftq,
  title={Loftq: Lora-fine-tuning-aware quantization for large language models},
  author={Li, Yixiao and Yu, Yifan and Liang, Chen and He, Pengcheng and Karampatziakis, Nikos and Chen, Weizhu and Zhao, Tuo},
  journal={arXiv preprint arXiv:2310.08659},
  year={2023}
}