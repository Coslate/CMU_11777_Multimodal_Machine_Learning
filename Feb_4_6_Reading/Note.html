<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.8/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.8/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.8/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:x,mm:K}=window,P=new x.Toolbar;P.attach(K);const F=P.render();F.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(F)})})()</script><script>((b,L,T,D)=>{const H=b();window.mm=H.Markmap.create("svg#mindmap",(L||H.deriveOptions)(D),T)})(()=>window.markmap,null,{"content":"📊 NuScenes-QA: A Multi-Modal Visual Question Answering Benchmark for Autonomous Driving","children":[{"content":"1️⃣ Introduction","children":[{"content":"<strong>Purpose:</strong> Introduce VQA (Visual Question Answering) in autonomous driving 🚗","children":[],"payload":{"tag":"li","lines":"3,4"}},{"content":"<strong>Challenges:</strong>","children":[{"content":"Multi-modal data (Images + LiDAR) 📷🗺️","children":[],"payload":{"tag":"li","lines":"5,6"}},{"content":"Multi-frame sequences 🎥","children":[],"payload":{"tag":"li","lines":"6,7"}},{"content":"Dynamic outdoor environments 🌳🚦","children":[],"payload":{"tag":"li","lines":"7,9"}}],"payload":{"tag":"li","lines":"4,9"}}],"payload":{"tag":"h2","lines":"2,3"}},{"content":"2️⃣ Related Works","children":[{"content":"<strong>VQA Benchmarks:</strong> VQA2.0, CLEVR, GQA (2D image-based) 📊","children":[],"payload":{"tag":"li","lines":"10,11"}},{"content":"<strong>3D VQA:</strong> 3DQA, ScanQA (limited to indoor static scenes) 🏠","children":[],"payload":{"tag":"li","lines":"11,12"}},{"content":"<strong>Autonomous Driving VQA:</strong> Talk2Car, Refer-KITTI (focus on object referral)","children":[],"payload":{"tag":"li","lines":"12,14"}}],"payload":{"tag":"h2","lines":"9,10"}},{"content":"3️⃣ NuScenes-QA Dataset 📚","children":[{"content":"🔍 Key Features:","children":[{"content":"Multi-modal (camera + LiDAR) 🎥🗺️","children":[],"payload":{"tag":"li","lines":"24,25"}},{"content":"Dynamic outdoor scenes 🌳","children":[],"payload":{"tag":"li","lines":"25,26"}},{"content":"High reasoning complexity 🧠","children":[],"payload":{"tag":"li","lines":"26,28"}}],"payload":{"tag":"h3","lines":"23,24"}}],"payload":{"tag":"h2","lines":"14,15"}},{"content":"4️⃣ Methodology ⚙️","children":[{"content":"📦 <strong>Framework Overview:</strong>","children":[{"content":"1. <strong>Feature Extraction Backbone:</strong>","children":[{"content":"Image-based: ResNet + FPN","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"LiDAR-based: 3D Sparse CNN","children":[],"payload":{"tag":"li","lines":"34,35"}}],"payload":{"tag":"li","lines":"32,35","listIndex":1}},{"content":"2. <strong>Region Proposal Module:</strong>","children":[{"content":"BEV (Bird’s Eye View) feature cropping 📏","children":[],"payload":{"tag":"li","lines":"36,37"}}],"payload":{"tag":"li","lines":"35,37","listIndex":2}},{"content":"3. <strong>QA-Head:</strong>","children":[{"content":"Models: BUTD, MCAN for multimodal fusion 🤖","children":[],"payload":{"tag":"li","lines":"38,40"}}],"payload":{"tag":"li","lines":"37,40","listIndex":3}}],"payload":{"tag":"h3","lines":"31,32"}},{"content":"🚀 <strong>BEV Feature Crop Strategy:</strong>","children":[{"content":"<strong>Problem:</strong> Cannot use traditional RoI pooling due to rotated 3D bounding boxes 🔄","children":[],"payload":{"tag":"li","lines":"41,42"}},{"content":"<strong>Solution:</strong>","children":[{"content":"<strong>Cross-Product Algorithm:</strong> Determines pixels inside rotated boxes ✅","children":[],"payload":{"tag":"li","lines":"43,44"}},{"content":"<strong>Circumscribed Rectangle:</strong> Simpler but includes irrelevant background ❌","children":[],"payload":{"tag":"li","lines":"44,46"}}],"payload":{"tag":"li","lines":"42,46"}}],"payload":{"tag":"h3","lines":"40,41"}},{"content":"❓ <strong>Questions &amp; Answers:</strong>","children":[{"content":"1. \n<p data-lines=\"47,48\"><strong>Why is the performance of the circumscribed rectangle slightly inferior to the rotated bounding box in BEV feature cropping?</strong></p>","children":[{"content":"<strong>Answer:</strong> The circumscribed rectangle includes extra background, causing feature smoothing, especially for elongated objects like buses 🚌.","children":[],"payload":{"tag":"li","lines":"48,50"}}],"payload":{"tag":"li","lines":"47,50","listIndex":1}},{"content":"2. \n<p data-lines=\"50,51\"><strong>What is the role of the cross-product algorithm in determining pixels inside the rotated bounding box for BEV feature cropping?</strong></p>","children":[{"content":"<strong>Answer:</strong> It accurately identifies pixels within rotated boxes, overcoming the limitations of standard RoI pooling in BEV.","children":[],"payload":{"tag":"li","lines":"51,53"}}],"payload":{"tag":"li","lines":"50,53","listIndex":2}},{"content":"3. \n<p data-lines=\"53,54\"><strong>Why does rotation in BEV cause misalignment?</strong></p>","children":[{"content":"<strong>Answer:</strong> Rotated bounding boxes do not align with the grid, making standard axis-aligned methods ineffective for precise feature extraction 📏.","children":[],"payload":{"tag":"li","lines":"54,56"}}],"payload":{"tag":"li","lines":"53,56","listIndex":3}},{"content":"4. \n<p data-lines=\"56,57\"><strong>What is a circumscribed rectangle?</strong></p>","children":[{"content":"<strong>Answer:</strong> The smallest axis-aligned rectangle that fully encloses a rotated box, often including irrelevant space 📦.","children":[],"payload":{"tag":"li","lines":"57,59"}}],"payload":{"tag":"li","lines":"56,59","listIndex":4}}],"payload":{"tag":"h3","lines":"46,47"}}],"payload":{"tag":"h2","lines":"30,31"}},{"content":"5️⃣ Experiments &amp; Results 🧪","children":[{"content":"📊 <strong>Key Findings:</strong>","children":[{"content":"Fusion models outperform single-modality models 💡","children":[],"payload":{"tag":"li","lines":"70,71"}},{"content":"Ground truth boxes significantly boost performance (+13.5%) ✅","children":[],"payload":{"tag":"li","lines":"71,72"}},{"content":"Mean pooling &gt; Max pooling for BEV features 📈","children":[],"payload":{"tag":"li","lines":"72,74"}}],"payload":{"tag":"h3","lines":"69,70"}}],"payload":{"tag":"h2","lines":"61,62"}},{"content":"6️⃣ Conclusion &amp; Future Work 🔍","children":[{"content":"\n<p data-lines=\"77,78\"><strong>Contributions:</strong></p>","children":[{"content":"First large-scale multi-modal VQA benchmark for autonomous driving 🚗","children":[],"payload":{"tag":"li","lines":"78,79"}},{"content":"Established strong baselines for future research 📊","children":[],"payload":{"tag":"li","lines":"79,81"}}],"payload":{"tag":"li","lines":"77,81"}},{"content":"\n<p data-lines=\"81,82\"><strong>Limitations:</strong></p>","children":[{"content":"Limited background-object relationship analysis ⚠️","children":[],"payload":{"tag":"li","lines":"82,83"}},{"content":"Focused on foreground objects only 🚧","children":[],"payload":{"tag":"li","lines":"83,85"}}],"payload":{"tag":"li","lines":"81,85"}},{"content":"\n<p data-lines=\"85,86\"><strong>Future Directions:</strong></p>","children":[{"content":"Incorporate object tracking and localization 🔎","children":[],"payload":{"tag":"li","lines":"86,87"}},{"content":"Use human-annotated scene graphs for richer semantic understanding 🤖","children":[],"payload":{"tag":"li","lines":"87,89"}}],"payload":{"tag":"li","lines":"85,89"}}],"payload":{"tag":"h2","lines":"76,77"}},{"content":"🌟 Summary","children":[],"payload":{"tag":"h2","lines":"91,92"}}],"payload":{"tag":"h1","lines":"0,1"}},null)</script>
</body>
</html>
